{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(  essay_id                                          full_text  score\n",
       " 0  000d118  Many people have car where they live. The thin...      3\n",
       " 1  000fe60  I am a scientist at NASA that is discussing th...      3\n",
       " 2  001ab80  People always wish they had the same technolog...      4\n",
       " 3  001bdc0  We all heard about Venus, the planet without a...      4\n",
       " 4  002ba53  Dear, State Senator\\n\\nThis is a letter to arg...      3,\n",
       " Index(['essay_id', 'full_text', 'score'], dtype='object'),\n",
       "   essay_id                                          full_text\n",
       " 0  000d118  Many people have car where they live. The thin...\n",
       " 1  000fe60  I am a scientist at NASA that is discussing th...\n",
       " 2  001ab80  People always wish they had the same technolog...,\n",
       " Index(['essay_id', 'full_text'], dtype='object'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-import libraries and redefine file paths\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "#  Preprocessing and Text Normalization\n",
    "\n",
    "\n",
    "# Define file paths\n",
    "train_file_path = r'Repositories\\\\Kaggle_Competitions\\\\Learning Agency Lab - Automated Essay Scoring 2.0\\\\learning-agency-lab-automated-essay-scoring-2\\\\train.csv'\n",
    "test_file_path = r'Repositories\\\\Kaggle_Competitions\\\\Learning Agency Lab - Automated Essay Scoring 2.0\\\\learning-agency-lab-automated-essay-scoring-2\\\\test.csv'\n",
    "\n",
    "# Load the data\n",
    "train_data = pd.read_csv(train_file_path)\n",
    "test_data = pd.read_csv(test_file_path)\n",
    "\n",
    "# Display the first few rows of each dataframe and their structure\n",
    "train_data.head(), train_data.columns, test_data.head(), test_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Text Normalization\n",
    "* Cleaning Text: Remove or normalize text artifacts like punctuation, capitalization, and special characters that might not contribute to essay scoring.\n",
    "* Tokenization and Lemmatization: Break down text into tokens (words or phrases) and reduce them to their base or dictionary form.\n",
    "* Stopword Removal: Consider the impact of removing common words that may not contribute to the overall meaning of the essay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\nickr\\OneDrive\\Υπολο\n",
      "[nltk_data]     γιστής\\Repositories\\Kaggle_Competitions\\Learning\n",
      "[nltk_data]     Agency Lab - Automated Essay Scoring 2.0\\...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\nickr\\OneDrive\\Υ\n",
      "[nltk_data]     πολογιστής\\Repositories\\Kaggle_Competitions\\Learning\n",
      "[nltk_data]     Agency Lab - Automated Essay Scoring 2.0\\...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\nickr\\OneDrive\\Υπο\n",
      "[nltk_data]     λογιστής\\Repositories\\Kaggle_Competitions\\Learning\n",
      "[nltk_data]     Agency Lab - Automated Essay Scoring 2.0\\...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up NLTK with local resources\n",
    "nltk.data.path.append('Repositories\\\\Kaggle_Competitions\\\\Learning Agency Lab - Automated Essay Scoring 2.0\\\\')  # Specifying a custom path for pre-loaded NLTK resources\n",
    "\n",
    "# Load NLTK resources necessary for the tasks\n",
    "nltk.download('punkt', download_dir='Repositories\\\\Kaggle_Competitions\\\\Learning Agency Lab - Automated Essay Scoring 2.0\\\\')  # Tokenizers\n",
    "nltk.download('stopwords', download_dir='Repositories\\\\Kaggle_Competitions\\\\Learning Agency Lab - Automated Essay Scoring 2.0\\\\')  # Stopwords\n",
    "nltk.download('wordnet', download_dir='Repositories\\\\Kaggle_Competitions\\\\Learning Agency Lab - Automated Essay Scoring 2.0\\\\') # Lemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                           full_text  \\\n",
       " 0  Many people have car where they live. The thin...   \n",
       " 1  I am a scientist at NASA that is discussing th...   \n",
       " 2  People always wish they had the same technolog...   \n",
       " 3  We all heard about Venus, the planet without a...   \n",
       " 4  Dear, State Senator\\n\\nThis is a letter to arg...   \n",
       " \n",
       "                                           clean_text  \n",
       " 0  many people car live thing know use car alot t...  \n",
       " 1  scientist nasa discussing face mar explaining ...  \n",
       " 2  people always wish technology seen movie best ...  \n",
       " 3  heard venus planet without almost oxygen earth...  \n",
       " 4  dear state senator letter argue favor keeping ...  ,\n",
       "                                            full_text  \\\n",
       " 0  Many people have car where they live. The thin...   \n",
       " 1  I am a scientist at NASA that is discussing th...   \n",
       " 2  People always wish they had the same technolog...   \n",
       " \n",
       "                                           clean_text  \n",
       " 0  many people car live thing know use car alot t...  \n",
       " 1  scientist nasa discussing face mar explaining ...  \n",
       " 2  people always wish technology seen movie best ...  )"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Initialize the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lower case\n",
    "    text = text.lower()\n",
    "    # Remove non-alphabetic characters and extra spaces\n",
    "    text = re.sub('[^a-z\\s]', ' ', text)\n",
    "    text = re.sub(' +', ' ', text).strip()\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Lemmatize words\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    # Join tokens back to string\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to both train and test data\n",
    "train_data['clean_text'] = train_data['full_text'].apply(preprocess_text)\n",
    "test_data['clean_text'] = test_data['full_text'].apply(preprocess_text)\n",
    "\n",
    "# Display first few rows to verify preprocessing\n",
    "train_data[['full_text', 'clean_text']].head(), test_data[['full_text', 'clean_text']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Feature Engineering\n",
    "* Linguistic Features: Extract features that represent the quality of writing, such as sentence complexity, vocabulary richness, grammar correctness, and coherence. Tools like the Natural Language Toolkit (NLTK) or spaCy can be helpful.\n",
    "* Text Embeddings: Use embeddings like Word2Vec, GloVe, or fastText to capture semantic relationships between words. Sentence and paragraph embeddings (e.g., from BERT or Sentence-BERT) can capture contextual nuances.\n",
    "* Syntactic Features: Parse trees and dependency graphs can help understand the syntactic structures of sentences, potentially indicating more complex writing abilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.04548181,\n",
       "         0.79017261, 0.04975925, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.04778838, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.0321821 , 0.1117614 ,\n",
       "         0.        , 0.19593373, 0.03828935, 0.        , 0.08631919,\n",
       "         0.03679849, 0.        , 0.        , 0.078892  , 0.0273742 ,\n",
       "         0.        , 0.        , 0.        , 0.03337794, 0.06588664,\n",
       "         0.        , 0.        , 0.        , 0.04189591, 0.        ,\n",
       "         0.11405601, 0.04082721, 0.03086056, 0.        , 0.13427865,\n",
       "         0.32417327, 0.        , 0.        , 0.1876218 , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.04384533, 0.04065578,\n",
       "         0.        , 0.15026438, 0.        , 0.        , 0.        ,\n",
       "         0.07776394, 0.        , 0.05145062, 0.        , 0.11402972,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.13661017, 0.10336992, 0.        ,\n",
       "         0.17038731, 0.        , 0.        , 0.        , 0.03322437,\n",
       "         0.        , 0.        , 0.04177855, 0.        , 0.09694338],\n",
       "        [0.        , 0.        , 0.        , 0.12031148, 0.        ,\n",
       "         0.06052403, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.08259009, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.13234917, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.38101442,\n",
       "         0.        , 0.        , 0.06502235, 0.0445639 , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.10085184, 0.        ,\n",
       "         0.10191284, 0.54652212, 0.        , 0.10924507, 0.1516248 ,\n",
       "         0.13987506, 0.        , 0.062744  , 0.04621983, 0.091236  ,\n",
       "         0.39450627, 0.        , 0.        , 0.        , 0.36208903,\n",
       "         0.        , 0.        , 0.17093564, 0.        , 0.03718826,\n",
       "         0.        , 0.        , 0.08350221, 0.        , 0.06603448,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.20270679, 0.07770461, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.06131636, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.07013054, 0.09458492, 0.09542712, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.06056744, 0.        , 0.10863886, 0.        ],\n",
       "        [0.03389527, 0.17204791, 0.        , 0.04384889, 0.        ,\n",
       "         0.06617602, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.69263231, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.06772703, 0.        , 0.        , 0.        , 0.11619127,\n",
       "         0.03865233, 0.24130051, 0.17688265, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.14446337, 0.        ,\n",
       "         0.        , 0.03987428, 0.        , 0.12181368, 0.05640431,\n",
       "         0.        , 0.        , 0.        , 0.02756745, 0.16336503,\n",
       "         0.        , 0.        , 0.04107377, 0.        , 0.04144604,\n",
       "         0.        , 0.03662262, 0.        , 0.02526802, 0.14963401,\n",
       "         0.        , 0.16234906, 0.        , 0.        , 0.        ,\n",
       "         0.05756236, 0.30907322, 0.04672459, 0.        , 0.06099159,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.04743942, 0.        , 0.07807622, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.11684862, 0.03352117, 0.0575491 ,\n",
       "         0.03295229, 0.        , 0.06314252, 0.02752881, 0.        ,\n",
       "         0.32421573, 0.        , 0.        , 0.02608463, 0.0238461 ,\n",
       "         0.        , 0.        , 0.        , 0.06239279, 0.05030353,\n",
       "         0.03407168, 0.03311174, 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.0914367 , 0.        ,\n",
       "         0.04599825, 0.05034357, 0.32843039, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.11305302, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.10058534, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.03347165, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.11497108, 0.04542132,\n",
       "         0.        , 0.        , 0.        , 0.04151312, 0.05761741,\n",
       "         0.        , 0.        , 0.        , 0.1053812 , 0.03466967,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.11007504,\n",
       "         0.        , 0.0429667 , 0.        , 0.        , 0.02826307,\n",
       "         0.05686019, 0.        , 0.        , 0.        , 0.45167574,\n",
       "         0.        , 0.        , 0.        , 0.04614299, 0.0427863 ,\n",
       "         0.        , 0.        , 0.        , 0.17716646, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.04580957, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.1065984 , 0.03594226, 0.        , 0.        ,\n",
       "         0.        , 0.73031196, 0.        , 0.        , 0.0699309 ,\n",
       "         0.        , 0.        , 0.        , 0.11008736, 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.03359203,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.56157192, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.099039  ,\n",
       "         0.1009494 , 0.56797115, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.02474935, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.12517673, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.02533476,\n",
       "         0.        , 0.03298508, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.02373303, 0.        , 0.02065316,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.19276898, 0.13886939, 0.        , 0.        , 0.12506386,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.17538706,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.46804938, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Function to calculate linguistic features\n",
    "def linguistic_features(text):\n",
    "    sentences = text.split('.')\n",
    "    sentence_lengths = [len(sentence.split()) for sentence in sentences if sentence != \"\"]\n",
    "    \n",
    "    # Average sentence length\n",
    "    avg_sentence_length = np.mean(sentence_lengths) if sentence_lengths else 0\n",
    "    \n",
    "    # Vocabulary richness: Type-Token Ratio (TTR)\n",
    "    words = text.split()\n",
    "    unique_words = set(words)\n",
    "    ttr = len(unique_words) / len(words) if words else 0\n",
    "    \n",
    "    return avg_sentence_length, ttr\n",
    "\n",
    "# Apply linguistic features calculation\n",
    "train_data['avg_sentence_length'], train_data['ttr'] = zip(*train_data['clean_text'].map(linguistic_features))\n",
    "test_data['avg_sentence_length'], test_data['ttr'] = zip(*test_data['clean_text'].map(linguistic_features))\n",
    "\n",
    "# Initialize a TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=100)  # Limit number of features to 100 for simplicity\n",
    "\n",
    "# Fit and transform the 'clean_text' column to create TF-IDF features\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(train_data['clean_text'])\n",
    "tfidf_test = tfidf_vectorizer.transform(test_data['clean_text'])\n",
    "\n",
    "# Example: Convert first 5 TF-IDF features of train data to dense format and display\n",
    "tfidf_train_dense_example = tfidf_train.todense()[:5]\n",
    "\n",
    "tfidf_train_dense_example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Selection and Ensemble Methods\n",
    "* Advanced NLP Models: Utilize state-of-the-art language models like BERT, GPT, or their variants (RoBERTa, DistilBERT, etc.) fine-tuned on the essay dataset.\n",
    "* Ensemble Methods: Combine predictions from multiple models to improve accuracy. Techniques like bagging, boosting, or stacking can be particularly effective, especially when combining models that capture different aspects of writing quality.\n",
    "* Custom Scoring Layers: For neural networks, consider designing custom layers or loss functions that directly optimize for the competition’s evaluation metric (Quadratic Weighted Kappa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.2.2\n",
      "Transformers version: 4.39.3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"Transformers version:\", transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8e90c18f778400eacf06d0f55271b5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nickr\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\nickr\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a89b890f1dbf4c3bbcc5d1fadb9c4c91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f70dda7b17c4a90be8065fc7c19e646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25628321b4b144caa8e02a66f91bb49a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b49b07d98504109b9d04f6cb2baafb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Example forward pass\u001b[39;00m\n\u001b[0;32m     21\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExample text input for BERT\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m(inputs\u001b[38;5;241m.\u001b[39minput_ids, attention_mask\u001b[38;5;241m=\u001b[39minputs\u001b[38;5;241m.\u001b[39mattention_mask)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(score)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "class BERTRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTRegressor, self).__init__()\n",
    "        self.bert = model_bert\n",
    "        self.regressor = nn.Linear(768, 1)  # Assuming the output of BERT is 768-dimensional\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        return self.regressor(last_hidden_state[:, 0, :])\n",
    "\n",
    "# Example forward pass\n",
    "inputs = tokenizer(\"Example text input for BERT\", return_tensors=\"pt\")\n",
    "score = model(inputs.input_ids, attention_mask=inputs.attention_mask)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nickr\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2688: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "MAX_LEN = 256  # Maximum length of tokens\n",
    "\n",
    "# GPU or CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "\n",
    "# Data Preparation Functions\n",
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    \"\"\" Converts text data into torch DataLoader. \"\"\"\n",
    "    token_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for text in df.full_text:\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=max_len,\n",
    "            add_special_tokens=True,\n",
    "            return_token_type_ids=False,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        token_ids.append(encoding['input_ids'])\n",
    "        attention_masks.append(encoding['attention_mask'])\n",
    "\n",
    "    token_ids = torch.cat(token_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    targets = torch.tensor(df.score.values)\n",
    "\n",
    "    dataset = TensorDataset(token_ids, attention_masks, targets)\n",
    "\n",
    "    sampler = RandomSampler(dataset)\n",
    "    loader = DataLoader(dataset, sampler=sampler, batch_size=batch_size)\n",
    "\n",
    "    return loader\n",
    "\n",
    "# Load data\n",
    "train_loader = create_data_loader(train_data, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_loader = create_data_loader(test_data, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "\n",
    "# BERT Model Setup for Regression\n",
    "class BERTRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTRegressor, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=False\n",
    "        )\n",
    "        output = self.drop(pooled_output)\n",
    "        return self.out(output)\n",
    "\n",
    "model = BERTRegressor()\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "loss_fn = nn.MSELoss().to(device)\n",
    "\n",
    "# Training Loop\n",
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for d in tqdm(data_loader):\n",
    "        input_ids = d[0].to(device)\n",
    "        attention_mask = d[1].to(device)\n",
    "        targets = d[2].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        outputs = outputs.squeeze()\n",
    "\n",
    "        loss = loss_fn(outputs, targets.float())\n",
    "\n",
    "        correct_predictions += torch.sqrt(loss) * torch.numel(targets)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
    "\n",
    "# Run Training\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model,\n",
    "        train_loader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        device,\n",
    "        scheduler,\n",
    "        len(train_data)\n",
    "    )\n",
    "\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
