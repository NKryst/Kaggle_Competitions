{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-import libraries and redefine file paths\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# Define file paths\n",
    "train_file_path = '/mnt/data/train.csv'\n",
    "test_file_path = '/mnt/data/test.csv'\n",
    "\n",
    "# Load the data\n",
    "train_data = pd.read_csv(train_file_path)\n",
    "test_data = pd.read_csv(test_file_path)\n",
    "\n",
    "# Display the first few rows of each dataframe and their structure\n",
    "train_data.head(), train_data.columns, test_data.head(), test_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up NLTK with local resources\n",
    "nltk.data.path.append('/mnt/data/nltk_data/')  # Specifying a custom path for pre-loaded NLTK resources\n",
    "\n",
    "# Load NLTK resources necessary for the tasks\n",
    "nltk.download('punkt', download_dir='/mnt/data/nltk_data/')  # Tokenizers\n",
    "nltk.download('stopwords', download_dir='/mnt/data/nltk_data/')  # Stopwords\n",
    "nltk.download('wordnet', download_dir='/mnt/data/nltk_data/')  # Lemmatizer\n",
    "\n",
    "# Initialize the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lower case\n",
    "    text = text.lower()\n",
    "    # Remove non-alphabetic characters and extra spaces\n",
    "    text = re.sub('[^a-z\\s]', ' ', text)\n",
    "    text = re.sub(' +', ' ', text).strip()\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Lemmatize words\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    # Join tokens back to string\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to both train and test data\n",
    "train_data['clean_text'] = train_data['full_text'].apply(preprocess_text)\n",
    "test_data['clean_text'] = test_data['full_text'].apply(preprocess_text)\n",
    "\n",
    "# Display first few rows to verify preprocessing\n",
    "train_data[['full_text', 'clean_text']].head(), test_data[['full_text', 'clean_text']].head()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
