{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(  essay_id                                          full_text  score\n",
       " 0  000d118  Many people have car where they live. The thin...      3\n",
       " 1  000fe60  I am a scientist at NASA that is discussing th...      3\n",
       " 2  001ab80  People always wish they had the same technolog...      4\n",
       " 3  001bdc0  We all heard about Venus, the planet without a...      4\n",
       " 4  002ba53  Dear, State Senator\\n\\nThis is a letter to arg...      3,\n",
       " Index(['essay_id', 'full_text', 'score'], dtype='object'),\n",
       "   essay_id                                          full_text\n",
       " 0  000d118  Many people have car where they live. The thin...\n",
       " 1  000fe60  I am a scientist at NASA that is discussing th...\n",
       " 2  001ab80  People always wish they had the same technolog...,\n",
       " Index(['essay_id', 'full_text'], dtype='object'))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-import libraries and redefine file paths\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "#  Preprocessing and Text Normalization\n",
    "\n",
    "\n",
    "# Define file paths\n",
    "train_file_path = r'\\\\Kaggle_Competitions\\\\Learning Agency Lab - Automated Essay Scoring 2.0\\\\learning-agency-lab-automated-essay-scoring-2\\\\train.csv'\n",
    "test_file_path = r'\\\\Kaggle_Competitions\\\\Learning Agency Lab - Automated Essay Scoring 2.0\\\\learning-agency-lab-automated-essay-scoring-2\\\\test.csv'\n",
    "\n",
    "# Load the data\n",
    "train_data = pd.read_csv(train_file_path)\n",
    "test_data = pd.read_csv(test_file_path)\n",
    "\n",
    "# Display the first few rows of each dataframe and their structure\n",
    "train_data.head(), train_data.columns, test_data.head(), test_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Text Normalization\n",
    "* Cleaning Text: Remove or normalize text artifacts like punctuation, capitalization, and special characters that might not contribute to essay scoring.\n",
    "* Tokenization and Lemmatization: Break down text into tokens (words or phrases) and reduce them to their base or dictionary form.\n",
    "* Stopword Removal: Consider the impact of removing common words that may not contribute to the overall meaning of the essay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\nickr\\OneDrive\\Υπολο\n",
      "[nltk_data]     γιστής\\Repositories\\Kaggle_Competitions\\Learning\n",
      "[nltk_data]     Agency Lab - Automated Essay Scoring 2.0\\...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\nickr\\OneDrive\\Υ\n",
      "[nltk_data]     πολογιστής\\Repositories\\Kaggle_Competitions\\Learning\n",
      "[nltk_data]     Agency Lab - Automated Essay Scoring 2.0\\...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\nickr\\OneDrive\\Υπο\n",
      "[nltk_data]     λογιστής\\Repositories\\Kaggle_Competitions\\Learning\n",
      "[nltk_data]     Agency Lab - Automated Essay Scoring 2.0\\...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Apply preprocessing to both train and test data\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfull_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(preprocess_text)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Display first few rows to verify preprocessing\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nickr\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4521\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4525\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4526\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4527\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4528\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4529\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4628\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nickr\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nickr\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1075\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1076\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\nickr\\anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[2], line 22\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     20\u001b[0m tokens \u001b[38;5;241m=\u001b[39m word_tokenize(text)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Remove stopwords\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43menglish\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     23\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Lemmatize words\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nickr\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py:21\u001b[0m, in \u001b[0;36mWordListCorpusReader.words\u001b[1;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwords\u001b[39m(\u001b[38;5;28mself\u001b[39m, fileids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ignore_lines_startswith\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m     20\u001b[0m         line\n\u001b[1;32m---> 21\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m line_tokenize(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileids\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstartswith(ignore_lines_startswith)\n\u001b[0;32m     23\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\nickr\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\api.py:218\u001b[0m, in \u001b[0;36mCorpusReader.raw\u001b[1;34m(self, fileids)\u001b[0m\n\u001b[0;32m    216\u001b[0m contents \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fileids:\n\u001b[1;32m--> 218\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m    219\u001b[0m         contents\u001b[38;5;241m.\u001b[39mappend(fp\u001b[38;5;241m.\u001b[39mread())\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concat(contents)\n",
      "File \u001b[1;32mc:\\Users\\nickr\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\api.py:231\u001b[0m, in \u001b[0;36mCorpusReader.open\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03mReturn an open stream that can be used to read the given file.\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;124;03mIf the file's encoding is not None, then the stream will\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;124;03m:param file: The file identifier of the file to read.\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    230\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding(file)\n\u001b[1;32m--> 231\u001b[0m stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_root\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen(encoding)\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\n",
      "File \u001b[1;32mc:\\Users\\nickr\\anaconda3\\lib\\site-packages\\nltk\\data.py:334\u001b[0m, in \u001b[0;36mFileSystemPathPointer.join\u001b[1;34m(self, fileid)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjoin\u001b[39m(\u001b[38;5;28mself\u001b[39m, fileid):\n\u001b[0;32m    333\u001b[0m     _path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path, fileid)\n\u001b[1;32m--> 334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFileSystemPathPointer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nickr\\anaconda3\\lib\\site-packages\\nltk\\compat.py:41\u001b[0m, in \u001b[0;36mpy3_data.<locals>._decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decorator\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     40\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args[\u001b[38;5;241m0\u001b[39m], add_py3_data(args[\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;241m+\u001b[39m args[\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minit_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nickr\\anaconda3\\lib\\site-packages\\nltk\\data.py:311\u001b[0m, in \u001b[0;36mFileSystemPathPointer.__init__\u001b[1;34m(self, _path)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;124;03mCreate a new path pointer for the given absolute path.\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \n\u001b[0;32m    307\u001b[0m \u001b[38;5;124;03m:raise IOError: If the given path does not exist.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    310\u001b[0m _path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(_path)\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_path\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo such file or directory: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m _path)\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path \u001b[38;5;241m=\u001b[39m _path\n",
      "File \u001b[1;32mc:\\Users\\nickr\\anaconda3\\lib\\genericpath.py:19\u001b[0m, in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 19\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Setting up NLTK with local resources\n",
    "nltk.data.path.append('\\\\Kaggle_Competitions\\\\Learning Agency Lab - Automated Essay Scoring 2.0\\\\')  # Specifying a custom path for pre-loaded NLTK resources\n",
    "\n",
    "# Load NLTK resources necessary for the tasks\n",
    "nltk.download('punkt', download_dir='\\\\Kaggle_Competitions\\\\Learning Agency Lab - Automated Essay Scoring 2.0\\\\')  # Tokenizers\n",
    "nltk.download('stopwords', download_dir='\\\\Kaggle_Competitions\\\\Learning Agency Lab - Automated Essay Scoring 2.0\\\\')  # Stopwords\n",
    "nltk.download('wordnet', download_dir='\\\\Kaggle_Competitions\\\\Learning Agency Lab - Automated Essay Scoring 2.0\\\\')  # Lemmatizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                           full_text  \\\n",
       " 0  Many people have car where they live. The thin...   \n",
       " 1  I am a scientist at NASA that is discussing th...   \n",
       " 2  People always wish they had the same technolog...   \n",
       " 3  We all heard about Venus, the planet without a...   \n",
       " 4  Dear, State Senator\\n\\nThis is a letter to arg...   \n",
       " \n",
       "                                           clean_text  \n",
       " 0  many people car live thing know use car alot t...  \n",
       " 1  scientist nasa discussing face mar explaining ...  \n",
       " 2  people always wish technology seen movie best ...  \n",
       " 3  heard venus planet without almost oxygen earth...  \n",
       " 4  dear state senator letter argue favor keeping ...  ,\n",
       "                                            full_text  \\\n",
       " 0  Many people have car where they live. The thin...   \n",
       " 1  I am a scientist at NASA that is discussing th...   \n",
       " 2  People always wish they had the same technolog...   \n",
       " \n",
       "                                           clean_text  \n",
       " 0  many people car live thing know use car alot t...  \n",
       " 1  scientist nasa discussing face mar explaining ...  \n",
       " 2  people always wish technology seen movie best ...  )"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lower case\n",
    "    text = text.lower()\n",
    "    # Remove non-alphabetic characters and extra spaces\n",
    "    text = re.sub('[^a-z\\s]', ' ', text)\n",
    "    text = re.sub(' +', ' ', text).strip()\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Lemmatize words\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    # Join tokens back to string\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to both train and test data\n",
    "train_data['clean_text'] = train_data['full_text'].apply(preprocess_text)\n",
    "test_data['clean_text'] = test_data['full_text'].apply(preprocess_text)\n",
    "\n",
    "# Display first few rows to verify preprocessing\n",
    "train_data[['full_text', 'clean_text']].head(), test_data[['full_text', 'clean_text']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Feature Engineering\n",
    "* Linguistic Features: Extract features that represent the quality of writing, such as sentence complexity, vocabulary richness, grammar correctness, and coherence. Tools like the Natural Language Toolkit (NLTK) or spaCy can be helpful.\n",
    "* Text Embeddings: Use embeddings like Word2Vec, GloVe, or fastText to capture semantic relationships between words. Sentence and paragraph embeddings (e.g., from BERT or Sentence-BERT) can capture contextual nuances.\n",
    "* Syntactic Features: Parse trees and dependency graphs can help understand the syntactic structures of sentences, potentially indicating more complex writing abilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.04548181,\n",
       "         0.79017261, 0.04975925, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.04778838, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.0321821 , 0.1117614 ,\n",
       "         0.        , 0.19593373, 0.03828935, 0.        , 0.08631919,\n",
       "         0.03679849, 0.        , 0.        , 0.078892  , 0.0273742 ,\n",
       "         0.        , 0.        , 0.        , 0.03337794, 0.06588664,\n",
       "         0.        , 0.        , 0.        , 0.04189591, 0.        ,\n",
       "         0.11405601, 0.04082721, 0.03086056, 0.        , 0.13427865,\n",
       "         0.32417327, 0.        , 0.        , 0.1876218 , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.04384533, 0.04065578,\n",
       "         0.        , 0.15026438, 0.        , 0.        , 0.        ,\n",
       "         0.07776394, 0.        , 0.05145062, 0.        , 0.11402972,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.13661017, 0.10336992, 0.        ,\n",
       "         0.17038731, 0.        , 0.        , 0.        , 0.03322437,\n",
       "         0.        , 0.        , 0.04177855, 0.        , 0.09694338],\n",
       "        [0.        , 0.        , 0.        , 0.12031148, 0.        ,\n",
       "         0.06052403, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.08259009, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.13234917, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.38101442,\n",
       "         0.        , 0.        , 0.06502235, 0.0445639 , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.10085184, 0.        ,\n",
       "         0.10191284, 0.54652212, 0.        , 0.10924507, 0.1516248 ,\n",
       "         0.13987506, 0.        , 0.062744  , 0.04621983, 0.091236  ,\n",
       "         0.39450627, 0.        , 0.        , 0.        , 0.36208903,\n",
       "         0.        , 0.        , 0.17093564, 0.        , 0.03718826,\n",
       "         0.        , 0.        , 0.08350221, 0.        , 0.06603448,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.20270679, 0.07770461, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.06131636, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.07013054, 0.09458492, 0.09542712, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.06056744, 0.        , 0.10863886, 0.        ],\n",
       "        [0.03389527, 0.17204791, 0.        , 0.04384889, 0.        ,\n",
       "         0.06617602, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.69263231, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.06772703, 0.        , 0.        , 0.        , 0.11619127,\n",
       "         0.03865233, 0.24130051, 0.17688265, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.14446337, 0.        ,\n",
       "         0.        , 0.03987428, 0.        , 0.12181368, 0.05640431,\n",
       "         0.        , 0.        , 0.        , 0.02756745, 0.16336503,\n",
       "         0.        , 0.        , 0.04107377, 0.        , 0.04144604,\n",
       "         0.        , 0.03662262, 0.        , 0.02526802, 0.14963401,\n",
       "         0.        , 0.16234906, 0.        , 0.        , 0.        ,\n",
       "         0.05756236, 0.30907322, 0.04672459, 0.        , 0.06099159,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.04743942, 0.        , 0.07807622, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.11684862, 0.03352117, 0.0575491 ,\n",
       "         0.03295229, 0.        , 0.06314252, 0.02752881, 0.        ,\n",
       "         0.32421573, 0.        , 0.        , 0.02608463, 0.0238461 ,\n",
       "         0.        , 0.        , 0.        , 0.06239279, 0.05030353,\n",
       "         0.03407168, 0.03311174, 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.0914367 , 0.        ,\n",
       "         0.04599825, 0.05034357, 0.32843039, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.11305302, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.10058534, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.03347165, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.11497108, 0.04542132,\n",
       "         0.        , 0.        , 0.        , 0.04151312, 0.05761741,\n",
       "         0.        , 0.        , 0.        , 0.1053812 , 0.03466967,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.11007504,\n",
       "         0.        , 0.0429667 , 0.        , 0.        , 0.02826307,\n",
       "         0.05686019, 0.        , 0.        , 0.        , 0.45167574,\n",
       "         0.        , 0.        , 0.        , 0.04614299, 0.0427863 ,\n",
       "         0.        , 0.        , 0.        , 0.17716646, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.04580957, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.1065984 , 0.03594226, 0.        , 0.        ,\n",
       "         0.        , 0.73031196, 0.        , 0.        , 0.0699309 ,\n",
       "         0.        , 0.        , 0.        , 0.11008736, 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.03359203,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.56157192, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.099039  ,\n",
       "         0.1009494 , 0.56797115, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.02474935, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.12517673, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.02533476,\n",
       "         0.        , 0.03298508, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.02373303, 0.        , 0.02065316,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.19276898, 0.13886939, 0.        , 0.        , 0.12506386,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.17538706,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.46804938, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Function to calculate linguistic features\n",
    "def linguistic_features(text):\n",
    "    sentences = text.split('.')\n",
    "    sentence_lengths = [len(sentence.split()) for sentence in sentences if sentence != \"\"]\n",
    "    \n",
    "    # Average sentence length\n",
    "    avg_sentence_length = np.mean(sentence_lengths) if sentence_lengths else 0\n",
    "    \n",
    "    # Vocabulary richness: Type-Token Ratio (TTR)\n",
    "    words = text.split()\n",
    "    unique_words = set(words)\n",
    "    ttr = len(unique_words) / len(words) if words else 0\n",
    "    \n",
    "    return avg_sentence_length, ttr\n",
    "\n",
    "# Apply linguistic features calculation\n",
    "train_data['avg_sentence_length'], train_data['ttr'] = zip(*train_data['clean_text'].map(linguistic_features))\n",
    "test_data['avg_sentence_length'], test_data['ttr'] = zip(*test_data['clean_text'].map(linguistic_features))\n",
    "\n",
    "# Initialize a TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=100)  # Limit number of features to 100 for simplicity\n",
    "\n",
    "# Fit and transform the 'clean_text' column to create TF-IDF features\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(train_data['clean_text'])\n",
    "tfidf_test = tfidf_vectorizer.transform(test_data['clean_text'])\n",
    "\n",
    "# Example: Convert first 5 TF-IDF features of train data to dense format and display\n",
    "tfidf_train_dense_example = tfidf_train.todense()[:5]\n",
    "\n",
    "tfidf_train_dense_example\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
